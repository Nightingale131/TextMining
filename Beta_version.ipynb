{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\48570\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\48570\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\48570\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import urllib.request as ur\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping information about articles from investing.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4 #How many main pages to scrap - define here\n",
    "\n",
    "url_common = 'https://www.investing.com/news/stock-market-news/'\n",
    "\n",
    "dates =[]\n",
    "source = [] #Source\n",
    "topics = [] #Topic\n",
    "links = [] #Linki\n",
    "short_content = []\n",
    "\n",
    "for i in tqdm(range(1,n)):\n",
    "    try: \n",
    "        req = ur.Request(url_common + str(i),headers={'User-Agent': 'Chrome/78'}) \n",
    "        html = ur.urlopen(req).read() \n",
    "    except:\n",
    "        pass\n",
    "    parsed = bs.BeautifulSoup(html,'lxml')\n",
    "    articles = parsed.find('div', {\"class\":'largeTitle'})\n",
    "    news  = articles.findAll('article', {\"class\":\"js-article-item articleItem\"})\n",
    "    news1 = articles.findAll('article', {\"data-content-type\":\"news\"})\n",
    "    for n in news:\n",
    "        topics.append(n.find('a', {'class':'title'}).text)\n",
    "        dates.append(n.find('span', {'class':'date'}).text[3:])\n",
    "        links.append(n.find('a').get('href'))\n",
    "    for n in news1:\n",
    "        topics.append(n.find('a', {'class':'title'}).text)\n",
    "        dates.append(n.find('span', {'class':'date'}).text[3:])\n",
    "        links.append(n.find('a').get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping content of atricles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "\n",
    "for i in tqdm(links):\n",
    "    if i[:3] !='htt':\n",
    "        url_common = 'https://www.investing.com/'\n",
    "    else:\n",
    "        url_common = ''\n",
    "    try:\n",
    "        req = ur.Request(url_common + i,headers={'User-Agent': 'Chrome/78'})\n",
    "        html = ur.urlopen(req).read()\n",
    "    except:\n",
    "        pass\n",
    "    parsed = bs.BeautifulSoup(html,'lxml')\n",
    "    paragraphs = parsed.find_all('p')\n",
    "    data = []\n",
    "    for p in paragraphs:\n",
    "        data.append(p.text)\n",
    "    df.iloc[m,2] = data\n",
    "    m = m+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the biggest comapany in every sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:09<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# declare lists to store the data\n",
    "name = []\n",
    "link = []\n",
    "symbol = []\n",
    "sector = []\n",
    "industries = ['ms_basic_materials', 'ms_communication_services', 'ms_consumer_cyclical', 'ms_consumer_defensive',\n",
    "              'ms_energy', 'ms_financial_services', 'ms_healthcare', 'ms_industrials', 'ms_real_estate',\n",
    "              'ms_technology', 'ms_utilities']\n",
    "\n",
    "for ind in tqdm(industries):\n",
    "    url = 'https://finance.yahoo.com/screener/predefined/' + str(ind)\n",
    "    req = requests.get(str(url)+'?offset=0&count=10',headers={'User-Agent': 'Chrome/78'})\n",
    "    parsed = bs.BeautifulSoup(req.content)\n",
    "    sectors = parsed.find(\"h1\", \"Fw(b) Fz(17px) D(ib)\").text\n",
    "    tr_list = parsed.findAll(\"tr\", \"simpTblRow\")[:1]\n",
    "    for i in tr_list:\n",
    "        names = i.find('a').get('title')\n",
    "        name.append(names)\n",
    "        lnk = i.find('a').get('href')\n",
    "        links = 'https://finance.yahoo.com' + lnk\n",
    "        link.append(links)\n",
    "        symbols = i.find('a').text\n",
    "        symbol.append(symbols)\n",
    "        sector.append(sectors)\n",
    "        \n",
    "dictio = {'symbol': symbol, 'name': name, 'link':link, 'sector':sector}\n",
    "df_industries = pd.DataFrame(dictio)\n",
    "df_industries.drop_duplicates(subset='name', keep='first', inplace=True)\n",
    "df_industries.drop_duplicates(subset='symbol', keep='first', inplace=True)\n",
    "df_industries.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BHP</td>\n",
       "      <td>BHP Group</td>\n",
       "      <td>https://finance.yahoo.com/quote/BHP?p=BHP</td>\n",
       "      <td>Basic Materials Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>https://finance.yahoo.com/quote/GOOGL?p=GOOGL</td>\n",
       "      <td>Communication Services Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>https://finance.yahoo.com/quote/AMZN?p=AMZN</td>\n",
       "      <td>Consumer Cyclical Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WMT</td>\n",
       "      <td>Walmart Inc.</td>\n",
       "      <td>https://finance.yahoo.com/quote/WMT?p=WMT</td>\n",
       "      <td>Consumer Defensive Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XOM</td>\n",
       "      <td>Exxon Mobil Corporation</td>\n",
       "      <td>https://finance.yahoo.com/quote/XOM?p=XOM</td>\n",
       "      <td>Energy Services Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HSBC-PA</td>\n",
       "      <td>HSBC Holdings plc ADR A 1/40PF A</td>\n",
       "      <td>https://finance.yahoo.com/quote/HSBC-PA?p=HSBC-PA</td>\n",
       "      <td>Financial Services Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JNJ</td>\n",
       "      <td>Johnson &amp; Johnson</td>\n",
       "      <td>https://finance.yahoo.com/quote/JNJ?p=JNJ</td>\n",
       "      <td>Healthcare Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LMT</td>\n",
       "      <td>Lockheed Martin Corporation</td>\n",
       "      <td>https://finance.yahoo.com/quote/LMT?p=LMT</td>\n",
       "      <td>Industrials Services Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AMT</td>\n",
       "      <td>American Tower Corporation (REIT)</td>\n",
       "      <td>https://finance.yahoo.com/quote/AMT?p=AMT</td>\n",
       "      <td>Real Estate Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corporation</td>\n",
       "      <td>https://finance.yahoo.com/quote/MSFT?p=MSFT</td>\n",
       "      <td>Technology Services Sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NEE</td>\n",
       "      <td>NextEra Energy, Inc.</td>\n",
       "      <td>https://finance.yahoo.com/quote/NEE?p=NEE</td>\n",
       "      <td>Utilities Sector</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol                               name  \\\n",
       "0       BHP                          BHP Group   \n",
       "1     GOOGL                      Alphabet Inc.   \n",
       "2      AMZN                   Amazon.com, Inc.   \n",
       "3       WMT                       Walmart Inc.   \n",
       "4       XOM            Exxon Mobil Corporation   \n",
       "5   HSBC-PA   HSBC Holdings plc ADR A 1/40PF A   \n",
       "6       JNJ                  Johnson & Johnson   \n",
       "7       LMT        Lockheed Martin Corporation   \n",
       "8       AMT  American Tower Corporation (REIT)   \n",
       "9      MSFT              Microsoft Corporation   \n",
       "10      NEE               NextEra Energy, Inc.   \n",
       "\n",
       "                                                 link  \\\n",
       "0           https://finance.yahoo.com/quote/BHP?p=BHP   \n",
       "1       https://finance.yahoo.com/quote/GOOGL?p=GOOGL   \n",
       "2         https://finance.yahoo.com/quote/AMZN?p=AMZN   \n",
       "3           https://finance.yahoo.com/quote/WMT?p=WMT   \n",
       "4           https://finance.yahoo.com/quote/XOM?p=XOM   \n",
       "5   https://finance.yahoo.com/quote/HSBC-PA?p=HSBC-PA   \n",
       "6           https://finance.yahoo.com/quote/JNJ?p=JNJ   \n",
       "7           https://finance.yahoo.com/quote/LMT?p=LMT   \n",
       "8           https://finance.yahoo.com/quote/AMT?p=AMT   \n",
       "9         https://finance.yahoo.com/quote/MSFT?p=MSFT   \n",
       "10          https://finance.yahoo.com/quote/NEE?p=NEE   \n",
       "\n",
       "                           sector  \n",
       "0          Basic Materials Sector  \n",
       "1   Communication Services Sector  \n",
       "2        Consumer Cyclical Sector  \n",
       "3       Consumer Defensive Sector  \n",
       "4          Energy Services Sector  \n",
       "5       Financial Services Sector  \n",
       "6               Healthcare Sector  \n",
       "7     Industrials Services Sector  \n",
       "8              Real Estate Sector  \n",
       "9      Technology Services Sector  \n",
       "10               Utilities Sector  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df', 'rb') as f:\n",
    "    data_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Keep Me Logged In', 'Advertisement', 'Login  Join ','\\xa0', 'Got a confidential news tip? We want to hear from you.',\n",
    " 'Sign up for free newsletters and get more CNBC delivered to your inbox',\n",
    " 'Get this delivered to your inbox, and more info about our products and services.\\xa0',\n",
    " '© 2020 CNBC LLC. All Rights Reserved. A Division of NBCUniversal',\n",
    " 'Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis.',\n",
    " 'Data also provided by ', 'Copyright © 2020 MarketWatch, Inc. All rights reserved.',\n",
    " '\\n                By using this site you agree to the\\n                Subscriber Agreement & Terms of Use Updated 03/26/2020\\n\\nPrivacy Notice and\\n                Cookie Notice.\\n',\n",
    " 'Do Not Sell My Personal Information.',\n",
    " '\\n          Intraday Data provided by FACTSET and subject to terms of use.\\n          Historical and current end-of-day data provided by FACTSET.\\n          All quotes are in local exchange time.\\n          Real-time last sale data for U.S. stock quotes reflect trades reported through Nasdaq only.\\n          Intraday data delayed at least 15 minutes or per exchange requirements.\\n        ',\n",
    " 'Create your free account',\n",
    " 'Already have an account? Login',\n",
    " 'By creating an account, you agree to theTerms of Service and acknowledge our Privacy Policy.',\n",
    " 'Log in to your account',\n",
    " \"Don't have a Benzinga account? Create one\", '© 2020 Benzinga.com. Benzinga does not provide investment advice. All rights reserved.', '\\n31/03/2020','© Copyright of Globes Publisher Itonut (1983) Ltd. 2020',  'Share with your friends ',\n",
    " 'BREAKING NEWS',\n",
    " 'ANALYTICS',\n",
    " 'COINS',\n",
    " '\\xa0',\n",
    " '\\xa0',\n",
    " 'ICO / STO / IEO',\n",
    " 'MINING',\n",
    " 'SHOP',\n",
    " 'No results matched your search',\n",
    " 'EXCHANGES',\n",
    " 'WALLETS',\n",
    " 'Copyright © 2020 CryptoDaily™', 'Your email address will not be published. Required fields are marked *', 'That...\\n  ',\n",
    " 'WSJ Membership',\n",
    " 'Customer Service',\n",
    " 'Tools & Features',\n",
    " 'Ads',\n",
    " 'More', '\\n31/03/2020 ',\n",
    " '\\n20/03/2020 ',\n",
    " '\\n20/03/2020 ',\n",
    " '\\n20/03/2020 ',\n",
    " '\\n19/03/2020 ',\n",
    " '\\n19/03/2020 ',\n",
    " '\\n18/03/2020 ',\n",
    " '\\n18/03/2020 ', 'WIRED is where tomorrow is realized. It is the essential source of information and ideas that make sense of a world in constant transformation. The WIRED conversation illuminates how technology is changing every aspect of our lives—from culture to business, science to design. The breakthroughs and innovations that we uncover lead to new ways of thinking, new connections, and new industries.',\n",
    " 'More From WIRED',\n",
    " 'Contact',\n",
    " '© 2020 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Your California Privacy Rights. Do Not Sell My Personal Information Wired may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices',\n",
    " 'Article Info', 'Supported by', \"Copyright © 2020. Market data provided is at least 10-minutes delayed and hosted by Barchart Solutions.Information is provided 'as-is' and solely for informational purposes, not for trading purposes or advice, and is delayed. To see all exchange delays and terms of use, please see disclaimer.\",\n",
    " 'Related Links:']\n",
    "  \n",
    "for i in list(data):\n",
    "    if i in words:\n",
    "    data.remove(i)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>links</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>/news/stock-market-news/hsbc-standard-chartere...</td>\n",
       "      <td>No results matched your searchBy Gina Lee Inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>/news/coronavirus/apple-doubles-china-donation...</td>\n",
       "      <td>No results matched your searchSHANGHAI (Reuter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>/news/stock-market-news/china-developers-raise...</td>\n",
       "      <td>No results matched your searchBy Clare Jim and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>https://invst.ly/qb2dk</td>\n",
       "      <td>AdvertisementSupported byDealBook Newsletter W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>https://invst.ly/qb26c</td>\n",
       "      <td>To revisit this article, select My⁠ ⁠Account, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>https://invst.ly/qb20c</td>\n",
       "      <td>Fashion clothing retailer Express Inc.\\n      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>https://invst.ly/qb200</td>\n",
       "      <td>BorgWarner Inc.\\n        BWA, \\n        -.%\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>https://invst.ly/qb1w3</td>\n",
       "      <td>British American Tobacco\\n        BTI, \\n     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>https://invst.ly/qb1vw</td>\n",
       "      <td>Ahead of Wisconsin’s primary on Tuesday, Democ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Apr 01, 2020</td>\n",
       "      <td>https://invst.ly/qb1s4</td>\n",
       "      <td>Keros Therapeutics\\n        KROS, \\n        \\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dates                                              links  \\\n",
       "0  Apr 01, 2020  /news/stock-market-news/hsbc-standard-chartere...   \n",
       "1  Apr 01, 2020  /news/coronavirus/apple-doubles-china-donation...   \n",
       "2  Apr 01, 2020  /news/stock-market-news/china-developers-raise...   \n",
       "3  Apr 01, 2020                             https://invst.ly/qb2dk   \n",
       "4  Apr 01, 2020                             https://invst.ly/qb26c   \n",
       "5  Apr 01, 2020                             https://invst.ly/qb20c   \n",
       "6  Apr 01, 2020                             https://invst.ly/qb200   \n",
       "7  Apr 01, 2020                             https://invst.ly/qb1w3   \n",
       "8  Apr 01, 2020                             https://invst.ly/qb1vw   \n",
       "9  Apr 01, 2020                             https://invst.ly/qb1s4   \n",
       "\n",
       "                                             content  \n",
       "0  No results matched your searchBy Gina Lee Inve...  \n",
       "1  No results matched your searchSHANGHAI (Reuter...  \n",
       "2  No results matched your searchBy Clare Jim and...  \n",
       "3  AdvertisementSupported byDealBook Newsletter W...  \n",
       "4  To revisit this article, select My⁠ ⁠Account, ...  \n",
       "5  Fashion clothing retailer Express Inc.\\n      ...  \n",
       "6  BorgWarner Inc.\\n        BWA, \\n        -.%\\n ...  \n",
       "7  British American Tobacco\\n        BTI, \\n     ...  \n",
       "8  Ahead of Wisconsin’s primary on Tuesday, Democ...  \n",
       "9  Keros Therapeutics\\n        KROS, \\n        \\n...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "data_all[\"content\"] = data_all[\"content\"].apply(lambda x: remove_punct(x))\n",
    "data_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_all['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15893"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.replace(np.nan, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.dates=pd.to_datetime(data_all['dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['content']= data_all['content'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>content_1</th>\n",
       "      <th>content_2</th>\n",
       "      <th>content_3</th>\n",
       "      <th>content_4</th>\n",
       "      <th>content_5</th>\n",
       "      <th>content_6</th>\n",
       "      <th>content_7</th>\n",
       "      <th>content_8</th>\n",
       "      <th>content_9</th>\n",
       "      <th>...</th>\n",
       "      <th>content_313</th>\n",
       "      <th>content_314</th>\n",
       "      <th>content_315</th>\n",
       "      <th>content_316</th>\n",
       "      <th>content_317</th>\n",
       "      <th>content_318</th>\n",
       "      <th>content_319</th>\n",
       "      <th>content_320</th>\n",
       "      <th>content_321</th>\n",
       "      <th>content_322</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>No results matched your searchBy David Shepard...</td>\n",
       "      <td>No results matched your search(Reuters) - Japa...</td>\n",
       "      <td>No results matched your searchBy Alexandra Alp...</td>\n",
       "      <td>No results matched your search(Reuters) - Elec...</td>\n",
       "      <td>No results matched your searchBy Greg Roumelio...</td>\n",
       "      <td>No results matched your searchInvesting.com – ...</td>\n",
       "      <td>No results matched your searchBy Gary McWillia...</td>\n",
       "      <td>No results matched your search(Reuters) - The ...</td>\n",
       "      <td>No results matched your searchBy Joshua Frankl...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>No results matched your search(Reuters) - The ...</td>\n",
       "      <td>No results matched your searchInvesting.com - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-09-29</td>\n",
       "      <td>No results matched your search(Bloomberg) -- F...</td>\n",
       "      <td>No results matched your searchBy Julie Zhu and...</td>\n",
       "      <td>No results matched your searchInvesting.com - ...</td>\n",
       "      <td>No results matched your searchBy Echo Wang and...</td>\n",
       "      <td>No results matched your searchBy Philip Blenki...</td>\n",
       "      <td>No results matched your searchBy Nichola Samin...</td>\n",
       "      <td>No results matched your searchInvesting.com – ...</td>\n",
       "      <td>No results matched your search(Reuters) -  Boe...</td>\n",
       "      <td>No results matched your searchBy Patpicha Tana...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>No results matched your search(Reuters) - The ...</td>\n",
       "      <td>No results matched your searchBy Moira Warburt...</td>\n",
       "      <td>No results matched your searchNEW YORK (Reuter...</td>\n",
       "      <td>No results matched your searchBy Brendan Piers...</td>\n",
       "      <td>No results matched your searchBy Huw JonesLOND...</td>\n",
       "      <td>No results matched your searchBy Anirban Sen(R...</td>\n",
       "      <td>No results matched your searchWASHINGTON (Reut...</td>\n",
       "      <td>No results matched your searchWASHINGTON (Reut...</td>\n",
       "      <td>No results matched your searchWASHINGTON (Reut...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>No results matched your search(Bloomberg) -- N...</td>\n",
       "      <td>No results matched your searchBy Paulina Duran...</td>\n",
       "      <td>No results matched your searchInvesting.com - ...</td>\n",
       "      <td>No results matched your searchBy Shubham Kalia...</td>\n",
       "      <td>No results matched your searchHONG KONG (Reute...</td>\n",
       "      <td>No results matched your searchBy Katanga Johns...</td>\n",
       "      <td>No results matched your search(Reuters) - Juul...</td>\n",
       "      <td>No results matched your searchHOUSTON (Reuters...</td>\n",
       "      <td>No results matched your searchBy David Shepard...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>No results matched your searchMEXICO CITY (Reu...</td>\n",
       "      <td>No results matched your search(Reuters) - Will...</td>\n",
       "      <td>No results matched your searchBy Krystal Hu(Re...</td>\n",
       "      <td>No results matched your search(Reuters) - OneW...</td>\n",
       "      <td>AdvertisementSupported byDr. Anthony Fauci, th...</td>\n",
       "      <td>Governments and investors woke up too late to ...</td>\n",
       "      <td>Keep Me Logged InThe stock market has gone so ...</td>\n",
       "      <td>Banks and financial-technology firms are start...</td>\n",
       "      <td>Keep Me Logged InMedical staffing firms are ra...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>No results matched your search(Bloomberg) -- I...</td>\n",
       "      <td>No results matched your searchBy David Shepard...</td>\n",
       "      <td>No results matched your search(Reuters) - The ...</td>\n",
       "      <td>No results matched your searchBy David Shepard...</td>\n",
       "      <td>No results matched your searchBy Gina LeeInves...</td>\n",
       "      <td>No results matched your searchBy Jamie FreedSY...</td>\n",
       "      <td>No results matched your searchTOKYO (Reuters) ...</td>\n",
       "      <td>No results matched your search(Bloomberg) -- S...</td>\n",
       "      <td>No results matched your searchMILAN (Reuters) ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>No results matched your search By Gina LeeInve...</td>\n",
       "      <td>No results matched your search(Reuters) - The ...</td>\n",
       "      <td>No results matched your searchBy Rachit Vats a...</td>\n",
       "      <td>No results matched your searchBy Jane Lanhee L...</td>\n",
       "      <td>No results matched your searchBy Tracy Rucinsk...</td>\n",
       "      <td>No results matched your searchBy Julie Steenhu...</td>\n",
       "      <td>No results matched your searchBy David Shepard...</td>\n",
       "      <td>No results matched your search(Reuters) - Lend...</td>\n",
       "      <td>No results matched your searchBy Joshua Frankl...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>No results matched your searchBy Gina LeeInves...</td>\n",
       "      <td>No results matched your search(Reuters) - Aust...</td>\n",
       "      <td>No results matched your searchBy Lawrence Whit...</td>\n",
       "      <td>No results matched your searchBy Greg Roumelio...</td>\n",
       "      <td>No results matched your searchBy Lewis Krausko...</td>\n",
       "      <td>No results matched your searchBy Simon JessopL...</td>\n",
       "      <td>No results matched your search(Reuters) - Macy...</td>\n",
       "      <td>No results matched your searchCHICAGO (Reuters...</td>\n",
       "      <td>No results matched your searchBy Kim Khan Inve...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>No results matched your searchBy Gina Lee Inve...</td>\n",
       "      <td>No results matched your searchSHANGHAI (Reuter...</td>\n",
       "      <td>No results matched your searchBy Clare Jim and...</td>\n",
       "      <td>AdvertisementSupported byDealBook Newsletter W...</td>\n",
       "      <td>To revisit this article, select My⁠ ⁠Account, ...</td>\n",
       "      <td>Fashion clothing retailer Express Inc.\\n      ...</td>\n",
       "      <td>BorgWarner Inc.\\n        BWA, \\n        -.%\\n ...</td>\n",
       "      <td>British American Tobacco\\n        BTI, \\n     ...</td>\n",
       "      <td>Ahead of Wisconsin’s primary on Tuesday, Democ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 323 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dates                                          content_1  \\\n",
       "0   2019-09-27  No results matched your searchBy David Shepard...   \n",
       "1   2019-09-28  No results matched your search(Reuters) - The ...   \n",
       "2   2019-09-29  No results matched your search(Bloomberg) -- F...   \n",
       "3   2019-09-30  No results matched your search(Reuters) - The ...   \n",
       "4   2019-10-01  No results matched your search(Bloomberg) -- N...   \n",
       "..         ...                                                ...   \n",
       "183 2020-03-28  No results matched your searchMEXICO CITY (Reu...   \n",
       "184 2020-03-29  No results matched your search(Bloomberg) -- I...   \n",
       "185 2020-03-30  No results matched your search By Gina LeeInve...   \n",
       "186 2020-03-31  No results matched your searchBy Gina LeeInves...   \n",
       "187 2020-04-01  No results matched your searchBy Gina Lee Inve...   \n",
       "\n",
       "                                             content_2  \\\n",
       "0    No results matched your search(Reuters) - Japa...   \n",
       "1    No results matched your searchInvesting.com - ...   \n",
       "2    No results matched your searchBy Julie Zhu and...   \n",
       "3    No results matched your searchBy Moira Warburt...   \n",
       "4    No results matched your searchBy Paulina Duran...   \n",
       "..                                                 ...   \n",
       "183  No results matched your search(Reuters) - Will...   \n",
       "184  No results matched your searchBy David Shepard...   \n",
       "185  No results matched your search(Reuters) - The ...   \n",
       "186  No results matched your search(Reuters) - Aust...   \n",
       "187  No results matched your searchSHANGHAI (Reuter...   \n",
       "\n",
       "                                             content_3  \\\n",
       "0    No results matched your searchBy Alexandra Alp...   \n",
       "1                                                  NaN   \n",
       "2    No results matched your searchInvesting.com - ...   \n",
       "3    No results matched your searchNEW YORK (Reuter...   \n",
       "4    No results matched your searchInvesting.com - ...   \n",
       "..                                                 ...   \n",
       "183  No results matched your searchBy Krystal Hu(Re...   \n",
       "184  No results matched your search(Reuters) - The ...   \n",
       "185  No results matched your searchBy Rachit Vats a...   \n",
       "186  No results matched your searchBy Lawrence Whit...   \n",
       "187  No results matched your searchBy Clare Jim and...   \n",
       "\n",
       "                                             content_4  \\\n",
       "0    No results matched your search(Reuters) - Elec...   \n",
       "1                                                  NaN   \n",
       "2    No results matched your searchBy Echo Wang and...   \n",
       "3    No results matched your searchBy Brendan Piers...   \n",
       "4    No results matched your searchBy Shubham Kalia...   \n",
       "..                                                 ...   \n",
       "183  No results matched your search(Reuters) - OneW...   \n",
       "184  No results matched your searchBy David Shepard...   \n",
       "185  No results matched your searchBy Jane Lanhee L...   \n",
       "186  No results matched your searchBy Greg Roumelio...   \n",
       "187  AdvertisementSupported byDealBook Newsletter W...   \n",
       "\n",
       "                                             content_5  \\\n",
       "0    No results matched your searchBy Greg Roumelio...   \n",
       "1                                                  NaN   \n",
       "2    No results matched your searchBy Philip Blenki...   \n",
       "3    No results matched your searchBy Huw JonesLOND...   \n",
       "4    No results matched your searchHONG KONG (Reute...   \n",
       "..                                                 ...   \n",
       "183  AdvertisementSupported byDr. Anthony Fauci, th...   \n",
       "184  No results matched your searchBy Gina LeeInves...   \n",
       "185  No results matched your searchBy Tracy Rucinsk...   \n",
       "186  No results matched your searchBy Lewis Krausko...   \n",
       "187  To revisit this article, select My⁠ ⁠Account, ...   \n",
       "\n",
       "                                             content_6  \\\n",
       "0    No results matched your searchInvesting.com – ...   \n",
       "1                                                  NaN   \n",
       "2    No results matched your searchBy Nichola Samin...   \n",
       "3    No results matched your searchBy Anirban Sen(R...   \n",
       "4    No results matched your searchBy Katanga Johns...   \n",
       "..                                                 ...   \n",
       "183  Governments and investors woke up too late to ...   \n",
       "184  No results matched your searchBy Jamie FreedSY...   \n",
       "185  No results matched your searchBy Julie Steenhu...   \n",
       "186  No results matched your searchBy Simon JessopL...   \n",
       "187  Fashion clothing retailer Express Inc.\\n      ...   \n",
       "\n",
       "                                             content_7  \\\n",
       "0    No results matched your searchBy Gary McWillia...   \n",
       "1                                                  NaN   \n",
       "2    No results matched your searchInvesting.com – ...   \n",
       "3    No results matched your searchWASHINGTON (Reut...   \n",
       "4    No results matched your search(Reuters) - Juul...   \n",
       "..                                                 ...   \n",
       "183  Keep Me Logged InThe stock market has gone so ...   \n",
       "184  No results matched your searchTOKYO (Reuters) ...   \n",
       "185  No results matched your searchBy David Shepard...   \n",
       "186  No results matched your search(Reuters) - Macy...   \n",
       "187  BorgWarner Inc.\\n        BWA, \\n        -.%\\n ...   \n",
       "\n",
       "                                             content_8  \\\n",
       "0    No results matched your search(Reuters) - The ...   \n",
       "1                                                  NaN   \n",
       "2    No results matched your search(Reuters) -  Boe...   \n",
       "3    No results matched your searchWASHINGTON (Reut...   \n",
       "4    No results matched your searchHOUSTON (Reuters...   \n",
       "..                                                 ...   \n",
       "183  Banks and financial-technology firms are start...   \n",
       "184  No results matched your search(Bloomberg) -- S...   \n",
       "185  No results matched your search(Reuters) - Lend...   \n",
       "186  No results matched your searchCHICAGO (Reuters...   \n",
       "187  British American Tobacco\\n        BTI, \\n     ...   \n",
       "\n",
       "                                             content_9  ... content_313  \\\n",
       "0    No results matched your searchBy Joshua Frankl...  ...         NaN   \n",
       "1                                                  NaN  ...         NaN   \n",
       "2    No results matched your searchBy Patpicha Tana...  ...         NaN   \n",
       "3    No results matched your searchWASHINGTON (Reut...  ...         NaN   \n",
       "4    No results matched your searchBy David Shepard...  ...         NaN   \n",
       "..                                                 ...  ...         ...   \n",
       "183  Keep Me Logged InMedical staffing firms are ra...  ...         NaN   \n",
       "184  No results matched your searchMILAN (Reuters) ...  ...         NaN   \n",
       "185  No results matched your searchBy Joshua Frankl...  ...         NaN   \n",
       "186  No results matched your searchBy Kim Khan Inve...  ...         NaN   \n",
       "187  Ahead of Wisconsin’s primary on Tuesday, Democ...  ...         NaN   \n",
       "\n",
       "    content_314 content_315 content_316 content_317 content_318 content_319  \\\n",
       "0           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "183         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "184         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "185         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "186         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "187         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "    content_320 content_321 content_322  \n",
       "0           NaN         NaN         NaN  \n",
       "1           NaN         NaN         NaN  \n",
       "2           NaN         NaN         NaN  \n",
       "3           NaN         NaN         NaN  \n",
       "4           NaN         NaN         NaN  \n",
       "..          ...         ...         ...  \n",
       "183         NaN         NaN         NaN  \n",
       "184         NaN         NaN         NaN  \n",
       "185         NaN         NaN         NaN  \n",
       "186         NaN         NaN         NaN  \n",
       "187         NaN         NaN         NaN  \n",
       "\n",
       "[188 rows x 323 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out = data_all.set_index(['dates',data_all.groupby(['dates']).cumcount()+1]).unstack().sort_index(level=1, axis=1)\n",
    "df_out.columns = df_out.columns.map('{0[0]}_{0[1]}'.format)\n",
    "df_out.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "class YahooFinanceHistory:\n",
    "    timeout = 2\n",
    "    crumb_link = 'https://finance.yahoo.com/quote/{0}/history?p={0}'\n",
    "    crumble_regex = r'CrumbStore\":{\"crumb\":\"(.*?)\"}'\n",
    "    quote_link = 'https://query1.finance.yahoo.com/v7/finance/download/{quote}?period1={dfrom}&period2={dto}&interval=1d&events=history&crumb={crumb}'\n",
    "\n",
    "    def __init__(self, symbol, days_back=7):\n",
    "        self.symbol = symbol\n",
    "        self.session = requests.Session()\n",
    "        self.dt = timedelta(days=days_back)\n",
    "\n",
    "#requesting crumb and cookie\n",
    "    def get_crumb(self):\n",
    "        response = self.session.get(self.crumb_link.format(self.symbol), timeout=self.timeout)\n",
    "        response.raise_for_status()\n",
    "        match = re.search(self.crumble_regex, response.text)\n",
    "        if not match:\n",
    "            raise ValueError('Could not get crumb from Yahoo Finance')\n",
    "        else:\n",
    "            self.crumb = match.group(1)\n",
    "\n",
    "#requesting data\n",
    "    def get_quote(self):\n",
    "        if not hasattr(self, 'crumb') or len(self.session.cookies) == 0:\n",
    "            self.get_crumb()\n",
    "        now = datetime.utcnow()\n",
    "        dateto = int(now.timestamp())\n",
    "        datefrom = int((now - self.dt).timestamp())\n",
    "        url = self.quote_link.format(quote=self.symbol, dfrom=datefrom, dto=dateto, crumb=self.crumb)\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(StringIO(response.text), parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = YahooFinanceHistory('BHP', days_back=191).get_quote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = df_v.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>49.380001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>49.009998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-02</td>\n",
       "      <td>47.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>47.869999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>48.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>36.689999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>35.869999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>37.900002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>36.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>38.900002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Close\n",
       "0   2019-09-30  49.380001\n",
       "1   2019-10-01  49.009998\n",
       "2   2019-10-02  47.680000\n",
       "3   2019-10-03  47.869999\n",
       "4   2019-10-04  48.410000\n",
       "..         ...        ...\n",
       "126 2020-03-31  36.689999\n",
       "127 2020-04-01  35.869999\n",
       "128 2020-04-02  37.900002\n",
       "129 2020-04-03  36.590000\n",
       "130 2020-04-06  38.900002\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_v = df_v.rename({'Date' : 'dates'}, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_v.Date=pd.to_datetime(df_v['dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df_v,df_out,on='dates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Anakin = SentimentIntensityAnalyzer()\n",
    "\n",
    "Anakin.polarity_scores(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)['compound']\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8381"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"Although a down market — spurred on by Friday’s jobs report — has Microsoft stock down about 1.5% on April 3, it remains about the only thing working. Even the dividend, 51 cents per share, looks payable. Its yield of 1.3% is close to that of the 30-year U.S. Treasury bond. Simple. The cloud. Not just the cloud, but cloud applications. These are the vital services that let millions work from home, that keep the economy alive. And these are the services seriously helping Microsoft stock right now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for i in range(1,323):\n",
    "    col = (\"content_{}\".format(i))\n",
    "    cols.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 322/322 [00:00<00:00, 2935.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(cols):\n",
    "    df3[col]= df3[col].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 322/322 [04:26<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "for col in tqdm(cols):\n",
    "    df3[col]= df3[col].apply(lambda x: sentiment_analyzer_scores(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>Close</th>\n",
       "      <th>content_1</th>\n",
       "      <th>content_2</th>\n",
       "      <th>content_3</th>\n",
       "      <th>content_4</th>\n",
       "      <th>content_5</th>\n",
       "      <th>content_6</th>\n",
       "      <th>content_7</th>\n",
       "      <th>content_8</th>\n",
       "      <th>...</th>\n",
       "      <th>content_313</th>\n",
       "      <th>content_314</th>\n",
       "      <th>content_315</th>\n",
       "      <th>content_316</th>\n",
       "      <th>content_317</th>\n",
       "      <th>content_318</th>\n",
       "      <th>content_319</th>\n",
       "      <th>content_320</th>\n",
       "      <th>content_321</th>\n",
       "      <th>content_322</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>49.380001</td>\n",
       "      <td>0.9764</td>\n",
       "      <td>-0.8871</td>\n",
       "      <td>0.6588</td>\n",
       "      <td>-0.4011</td>\n",
       "      <td>0.9680</td>\n",
       "      <td>0.6063</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>49.009998</td>\n",
       "      <td>-0.7649</td>\n",
       "      <td>-0.6906</td>\n",
       "      <td>-0.9813</td>\n",
       "      <td>-0.8610</td>\n",
       "      <td>-0.9670</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.9737</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-02</td>\n",
       "      <td>47.680000</td>\n",
       "      <td>0.6793</td>\n",
       "      <td>-0.7840</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9756</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>-0.9964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>47.869999</td>\n",
       "      <td>-0.7537</td>\n",
       "      <td>0.7712</td>\n",
       "      <td>0.8356</td>\n",
       "      <td>0.9134</td>\n",
       "      <td>0.5502</td>\n",
       "      <td>-0.8762</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>0.6466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>48.410000</td>\n",
       "      <td>-0.2584</td>\n",
       "      <td>-0.9450</td>\n",
       "      <td>0.5707</td>\n",
       "      <td>0.2003</td>\n",
       "      <td>-0.9563</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>37.849998</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>0.9501</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>35.590000</td>\n",
       "      <td>0.3382</td>\n",
       "      <td>-0.9684</td>\n",
       "      <td>0.7521</td>\n",
       "      <td>0.8881</td>\n",
       "      <td>0.4753</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>-0.4574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>37.049999</td>\n",
       "      <td>-0.9329</td>\n",
       "      <td>-0.8687</td>\n",
       "      <td>-0.8071</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.6696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>0.9348</td>\n",
       "      <td>-0.2018</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0.9624</td>\n",
       "      <td>0.5843</td>\n",
       "      <td>0.8016</td>\n",
       "      <td>-0.7139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>35.869999</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.4914</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9339</td>\n",
       "      <td>0.6952</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dates      Close  content_1  content_2  content_3  content_4  \\\n",
       "0   2019-09-30  49.380001     0.9764    -0.8871     0.6588    -0.4011   \n",
       "1   2019-10-01  49.009998    -0.7649    -0.6906    -0.9813    -0.8610   \n",
       "2   2019-10-02  47.680000     0.6793    -0.7840     0.8511     0.9945   \n",
       "3   2019-10-03  47.869999    -0.7537     0.7712     0.8356     0.9134   \n",
       "4   2019-10-04  48.410000    -0.2584    -0.9450     0.5707     0.2003   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "123 2020-03-26  37.849998     0.7177     0.9501     0.7506     0.9918   \n",
       "124 2020-03-27  35.590000     0.3382    -0.9684     0.7521     0.8881   \n",
       "125 2020-03-30  37.049999    -0.9329    -0.8687    -0.8071     0.9843   \n",
       "126 2020-03-31  36.689999     0.9348    -0.2018     0.9893     0.9746   \n",
       "127 2020-04-01  35.869999     0.9565     0.4914     0.1860     0.9849   \n",
       "\n",
       "     content_5  content_6  content_7  content_8  ...  content_313  \\\n",
       "0       0.9680     0.6063     0.6360     0.7500  ...          0.0   \n",
       "1      -0.9670     0.9845     0.9737     0.8080  ...          0.0   \n",
       "2       0.9916     0.9756     0.9840    -0.9964  ...          0.0   \n",
       "3       0.5502    -0.8762     0.9571     0.6466  ...          0.0   \n",
       "4      -0.9563     0.9688     0.9259     0.8268  ...          0.0   \n",
       "..         ...        ...        ...        ...  ...          ...   \n",
       "123     0.9923     0.9632     0.9848     0.9930  ...          0.0   \n",
       "124     0.4753     0.9823     0.9449    -0.4574  ...          0.0   \n",
       "125     0.7345     0.9980     0.9432     0.6696  ...          0.0   \n",
       "126     0.9624     0.5843     0.8016    -0.7139  ...          0.0   \n",
       "127     0.9986     0.9339     0.6952     0.7096  ...          0.0   \n",
       "\n",
       "     content_314  content_315  content_316  content_317  content_318  \\\n",
       "0            0.0          0.0          0.0          0.0          0.0   \n",
       "1            0.0          0.0          0.0          0.0          0.0   \n",
       "2            0.0          0.0          0.0          0.0          0.0   \n",
       "3            0.0          0.0          0.0          0.0          0.0   \n",
       "4            0.0          0.0          0.0          0.0          0.0   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "123          0.0          0.0          0.0          0.0          0.0   \n",
       "124          0.0          0.0          0.0          0.0          0.0   \n",
       "125          0.0          0.0          0.0          0.0          0.0   \n",
       "126          0.0          0.0          0.0          0.0          0.0   \n",
       "127          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "     content_319  content_320  content_321  content_322  \n",
       "0            0.0          0.0          0.0          0.0  \n",
       "1            0.0          0.0          0.0          0.0  \n",
       "2            0.0          0.0          0.0          0.0  \n",
       "3            0.0          0.0          0.0          0.0  \n",
       "4            0.0          0.0          0.0          0.0  \n",
       "..           ...          ...          ...          ...  \n",
       "123          0.0          0.0          0.0          0.0  \n",
       "124          0.0          0.0          0.0          0.0  \n",
       "125          0.0          0.0          0.0          0.0  \n",
       "126          0.0          0.0          0.0          0.0  \n",
       "127          0.0          0.0          0.0          0.0  \n",
       "\n",
       "[128 rows x 324 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_comp = []\n",
    "for i in range(0,len(df3)):\n",
    "    a = df3.loc[i,cols].tolist()\n",
    "    n = 0\n",
    "    i= 0 \n",
    "    length = len(a)  \n",
    "    while(i<length):\n",
    "        if(a[i]==n):\n",
    "            a.remove(a[i])\n",
    "            length = length -1  \n",
    "            continue\n",
    "        i = i+1\n",
    "    weighted_comp.append(np.average(a))\n",
    "\n",
    "df3['compound_mean'] = weighted_comp\n",
    "df3 = df3.drop(cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating label of \"up\" and \"down\" for prices for next day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['label'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-24c234d63115>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4402\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4404\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4405\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 128"
     ]
    }
   ],
   "source": [
    "#label oznacza, czy cena akcji tego dnia wzrośsła czy spadła\n",
    "#tutaj tak samo error, ale się label dobrze robi, więc czill xd\n",
    "m = 1\n",
    "for i in df3.Close:\n",
    "    if i < df3.Close[m]:\n",
    "        df3.label[m-1] = 1\n",
    "    else:\n",
    "        df3.label[m-1] = 0\n",
    "    m = m+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(df3.index[127], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>Close</th>\n",
       "      <th>compound_mean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>49.380001</td>\n",
       "      <td>0.431213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>49.009998</td>\n",
       "      <td>0.146505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-02</td>\n",
       "      <td>47.680000</td>\n",
       "      <td>0.211254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>47.869999</td>\n",
       "      <td>0.352498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>48.410000</td>\n",
       "      <td>0.387687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>37.320000</td>\n",
       "      <td>0.584710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>37.849998</td>\n",
       "      <td>0.571708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>35.590000</td>\n",
       "      <td>0.507320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>37.049999</td>\n",
       "      <td>0.438026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dates      Close  compound_mean label\n",
       "0   2019-09-30  49.380001       0.431213     0\n",
       "1   2019-10-01  49.009998       0.146505     0\n",
       "2   2019-10-02  47.680000       0.211254     1\n",
       "3   2019-10-03  47.869999       0.352498     1\n",
       "4   2019-10-04  48.410000       0.387687     0\n",
       "..         ...        ...            ...   ...\n",
       "122 2020-03-25  37.320000       0.584710     1\n",
       "123 2020-03-26  37.849998       0.571708     0\n",
       "124 2020-03-27  35.590000       0.507320     1\n",
       "125 2020-03-30  37.049999       0.438026     0\n",
       "126 2020-03-31  36.689999       0.409500     0\n",
       "\n",
       "[127 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df3', 'wb') as f:\n",
    "    pickle.dump(df3,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df3', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.asarray(df[['compound_mean', 'Close']])\n",
    "Y = np.asarray(df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(2,) ))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95 samples, validate on 32 samples\n",
      "Epoch 1/30\n",
      "95/95 [==============================] - 0s 3ms/step - loss: 1.1999 - accuracy: 0.5368 - val_loss: 0.8169 - val_accuracy: 0.4062\n",
      "Epoch 2/30\n",
      "95/95 [==============================] - 0s 340us/step - loss: 1.7436 - accuracy: 0.4737 - val_loss: 1.1396 - val_accuracy: 0.4062\n",
      "Epoch 3/30\n",
      "95/95 [==============================] - 0s 239us/step - loss: 1.3991 - accuracy: 0.4947 - val_loss: 0.6761 - val_accuracy: 0.5938\n",
      "Epoch 4/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.8395 - accuracy: 0.5895 - val_loss: 0.7001 - val_accuracy: 0.5938\n",
      "Epoch 5/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 1.1634 - accuracy: 0.4632 - val_loss: 0.6839 - val_accuracy: 0.5938\n",
      "Epoch 6/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.9303 - accuracy: 0.5158 - val_loss: 0.8573 - val_accuracy: 0.4062\n",
      "Epoch 7/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 1.1117 - accuracy: 0.5158 - val_loss: 0.6736 - val_accuracy: 0.5938\n",
      "Epoch 8/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.9845 - accuracy: 0.5158 - val_loss: 0.6793 - val_accuracy: 0.5938\n",
      "Epoch 9/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 1.0512 - accuracy: 0.4105 - val_loss: 0.7019 - val_accuracy: 0.4062\n",
      "Epoch 10/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.8407 - accuracy: 0.4737 - val_loss: 0.8119 - val_accuracy: 0.4062\n",
      "Epoch 11/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.8819 - accuracy: 0.5053 - val_loss: 0.7334 - val_accuracy: 0.4062\n",
      "Epoch 12/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.8487 - accuracy: 0.4947 - val_loss: 0.6992 - val_accuracy: 0.4062\n",
      "Epoch 13/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.8258 - accuracy: 0.4526 - val_loss: 0.6923 - val_accuracy: 0.5938\n",
      "Epoch 14/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.8091 - accuracy: 0.4316 - val_loss: 0.7295 - val_accuracy: 0.4062\n",
      "Epoch 15/30\n",
      "95/95 [==============================] - 0s 210us/step - loss: 0.7147 - accuracy: 0.5368 - val_loss: 0.7652 - val_accuracy: 0.4062\n",
      "Epoch 16/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.7897 - accuracy: 0.5053 - val_loss: 0.7669 - val_accuracy: 0.4062\n",
      "Epoch 17/30\n",
      "95/95 [==============================] - 0s 241us/step - loss: 0.7060 - accuracy: 0.5684 - val_loss: 0.7111 - val_accuracy: 0.4062\n",
      "Epoch 18/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.7914 - accuracy: 0.5053 - val_loss: 0.6902 - val_accuracy: 0.5938\n",
      "Epoch 19/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.8241 - accuracy: 0.4737 - val_loss: 0.6938 - val_accuracy: 0.4062\n",
      "Epoch 20/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.7335 - accuracy: 0.5368 - val_loss: 0.7088 - val_accuracy: 0.4062\n",
      "Epoch 21/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.7137 - accuracy: 0.5368 - val_loss: 0.7121 - val_accuracy: 0.4062\n",
      "Epoch 22/30\n",
      "95/95 [==============================] - 0s 210us/step - loss: 0.7662 - accuracy: 0.5368 - val_loss: 0.6943 - val_accuracy: 0.4062\n",
      "Epoch 23/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.7374 - accuracy: 0.5474 - val_loss: 0.6799 - val_accuracy: 0.5938\n",
      "Epoch 24/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.7737 - accuracy: 0.4105 - val_loss: 0.6733 - val_accuracy: 0.5938\n",
      "Epoch 25/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.7813 - accuracy: 0.4211 - val_loss: 0.6759 - val_accuracy: 0.5938\n",
      "Epoch 26/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.7332 - accuracy: 0.4737 - val_loss: 0.6891 - val_accuracy: 0.5938\n",
      "Epoch 27/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.7402 - accuracy: 0.5579 - val_loss: 0.6982 - val_accuracy: 0.4062\n",
      "Epoch 28/30\n",
      "95/95 [==============================] - 0s 220us/step - loss: 0.7155 - accuracy: 0.4526 - val_loss: 0.7046 - val_accuracy: 0.4062\n",
      "Epoch 29/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.7440 - accuracy: 0.4316 - val_loss: 0.6993 - val_accuracy: 0.4062\n",
      "Epoch 30/30\n",
      "95/95 [==============================] - 0s 231us/step - loss: 0.6970 - accuracy: 0.5895 - val_loss: 0.6869 - val_accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data= (X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/32 [==============================] - 0s 62us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6868644952774048, 0.59375]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.asarray(df[['compound_mean']])\n",
    "Y = np.asarray(df['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95 samples, validate on 32 samples\n",
      "Epoch 1/30\n",
      "95/95 [==============================] - 1s 6ms/step - loss: 0.6927 - accuracy: 0.5263 - val_loss: 0.6927 - val_accuracy: 0.5625\n",
      "Epoch 2/30\n",
      "95/95 [==============================] - 0s 267us/step - loss: 0.6945 - accuracy: 0.4842 - val_loss: 0.6925 - val_accuracy: 0.5312\n",
      "Epoch 3/30\n",
      "95/95 [==============================] - 0s 273us/step - loss: 0.6908 - accuracy: 0.5368 - val_loss: 0.6930 - val_accuracy: 0.5312\n",
      "Epoch 4/30\n",
      "95/95 [==============================] - 0s 241us/step - loss: 0.6929 - accuracy: 0.4842 - val_loss: 0.6931 - val_accuracy: 0.5312\n",
      "Epoch 5/30\n",
      "95/95 [==============================] - 0s 241us/step - loss: 0.6910 - accuracy: 0.5158 - val_loss: 0.6933 - val_accuracy: 0.5312\n",
      "Epoch 6/30\n",
      "95/95 [==============================] - 0s 273us/step - loss: 0.6926 - accuracy: 0.5158 - val_loss: 0.6934 - val_accuracy: 0.5312\n",
      "Epoch 7/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6969 - accuracy: 0.5053 - val_loss: 0.6932 - val_accuracy: 0.5312\n",
      "Epoch 8/30\n",
      "95/95 [==============================] - 0s 273us/step - loss: 0.6890 - accuracy: 0.5474 - val_loss: 0.6928 - val_accuracy: 0.5312\n",
      "Epoch 9/30\n",
      "95/95 [==============================] - 0s 283us/step - loss: 0.6821 - accuracy: 0.6105 - val_loss: 0.6929 - val_accuracy: 0.5312\n",
      "Epoch 10/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6968 - accuracy: 0.5368 - val_loss: 0.6930 - val_accuracy: 0.5312\n",
      "Epoch 11/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6937 - accuracy: 0.5158 - val_loss: 0.6932 - val_accuracy: 0.5312\n",
      "Epoch 12/30\n",
      "95/95 [==============================] - 0s 273us/step - loss: 0.6964 - accuracy: 0.5053 - val_loss: 0.6934 - val_accuracy: 0.5312\n",
      "Epoch 13/30\n",
      "95/95 [==============================] - 0s 262us/step - loss: 0.6932 - accuracy: 0.4842 - val_loss: 0.6936 - val_accuracy: 0.5312\n",
      "Epoch 14/30\n",
      "95/95 [==============================] - 0s 262us/step - loss: 0.6994 - accuracy: 0.4632 - val_loss: 0.6938 - val_accuracy: 0.5312\n",
      "Epoch 15/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6888 - accuracy: 0.5053 - val_loss: 0.6940 - val_accuracy: 0.5312\n",
      "Epoch 16/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6991 - accuracy: 0.4632 - val_loss: 0.6940 - val_accuracy: 0.5312\n",
      "Epoch 17/30\n",
      "95/95 [==============================] - 0s 241us/step - loss: 0.6905 - accuracy: 0.4947 - val_loss: 0.6939 - val_accuracy: 0.5312\n",
      "Epoch 18/30\n",
      "95/95 [==============================] - 0s 262us/step - loss: 0.6929 - accuracy: 0.4842 - val_loss: 0.6939 - val_accuracy: 0.5312\n",
      "Epoch 19/30\n",
      "95/95 [==============================] - 0s 273us/step - loss: 0.6874 - accuracy: 0.5368 - val_loss: 0.6939 - val_accuracy: 0.5312\n",
      "Epoch 20/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6946 - accuracy: 0.4947 - val_loss: 0.6938 - val_accuracy: 0.5312\n",
      "Epoch 21/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6993 - accuracy: 0.4316 - val_loss: 0.6937 - val_accuracy: 0.5312\n",
      "Epoch 22/30\n",
      "95/95 [==============================] - 0s 274us/step - loss: 0.6975 - accuracy: 0.4737 - val_loss: 0.6934 - val_accuracy: 0.5312\n",
      "Epoch 23/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6949 - accuracy: 0.4632 - val_loss: 0.6931 - val_accuracy: 0.5312\n",
      "Epoch 24/30\n",
      "95/95 [==============================] - 0s 262us/step - loss: 0.6935 - accuracy: 0.4211 - val_loss: 0.6929 - val_accuracy: 0.5312\n",
      "Epoch 25/30\n",
      "95/95 [==============================] - 0s 262us/step - loss: 0.6984 - accuracy: 0.4421 - val_loss: 0.6928 - val_accuracy: 0.5312\n",
      "Epoch 26/30\n",
      "95/95 [==============================] - 0s 262us/step - loss: 0.6999 - accuracy: 0.3789 - val_loss: 0.6926 - val_accuracy: 0.5312\n",
      "Epoch 27/30\n",
      "95/95 [==============================] - 0s 241us/step - loss: 0.6921 - accuracy: 0.5053 - val_loss: 0.6924 - val_accuracy: 0.5312\n",
      "Epoch 28/30\n",
      "95/95 [==============================] - 0s 252us/step - loss: 0.6905 - accuracy: 0.5053 - val_loss: 0.6924 - val_accuracy: 0.5312\n",
      "Epoch 29/30\n",
      "95/95 [==============================] - 0s 241us/step - loss: 0.6914 - accuracy: 0.5263 - val_loss: 0.6922 - val_accuracy: 0.5312\n",
      "Epoch 30/30\n",
      "95/95 [==============================] - 0s 236us/step - loss: 0.7001 - accuracy: 0.5158 - val_loss: 0.6921 - val_accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(1,) ))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data= (X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/32 [==============================] - 0s 92us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6920510530471802, 0.53125]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"batch_size\": 2,  \n",
    "    \"epochs\": 500,\n",
    "    \"lr\": 0.00010000,\n",
    "    \"time_steps\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "stime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat,batch_size):\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if no_of_rows_drop > 0:\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index):\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    print(\"dim_0\",dim_0)\n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = [\"Close\",'compound_mean']\n",
    "df_train, df_test = train_test_split(df, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_0 96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2006c57fffcb4d188c8cc8c20c6af0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_t, y_t = build_timeseries(x_train, 0)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_0 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed206ee6a0b448c9ab6d95942b399b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 0)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 10 samples\n",
      "Epoch 1/500\n",
      " - 2s - loss: 0.0697 - val_loss: 2.7330\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.73298, saving model to best_model.h5\n",
      "Epoch 2/500\n",
      " - 1s - loss: 0.0681 - val_loss: 2.7275\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.73298 to 2.72748, saving model to best_model.h5\n",
      "Epoch 3/500\n",
      " - 1s - loss: 0.0664 - val_loss: 2.7181\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.72748 to 2.71806, saving model to best_model.h5\n",
      "Epoch 4/500\n",
      " - 1s - loss: 0.0637 - val_loss: 2.7051\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.71806 to 2.70510, saving model to best_model.h5\n",
      "Epoch 5/500\n",
      " - 1s - loss: 0.0604 - val_loss: 2.6907\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.70510 to 2.69073, saving model to best_model.h5\n",
      "Epoch 6/500\n",
      " - 1s - loss: 0.0579 - val_loss: 2.6728\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.69073 to 2.67276, saving model to best_model.h5\n",
      "Epoch 7/500\n",
      " - 1s - loss: 0.0571 - val_loss: 2.6520\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.67276 to 2.65202, saving model to best_model.h5\n",
      "Epoch 8/500\n",
      " - 1s - loss: 0.0562 - val_loss: 2.6326\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.65202 to 2.63258, saving model to best_model.h5\n",
      "Epoch 9/500\n",
      " - 1s - loss: 0.0528 - val_loss: 2.5924\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.63258 to 2.59245, saving model to best_model.h5\n",
      "Epoch 10/500\n",
      " - 1s - loss: 0.0482 - val_loss: 2.5474\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.59245 to 2.54744, saving model to best_model.h5\n",
      "Epoch 11/500\n",
      " - 1s - loss: 0.0425 - val_loss: 2.4970\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.54744 to 2.49705, saving model to best_model.h5\n",
      "Epoch 12/500\n",
      " - 1s - loss: 0.0381 - val_loss: 2.4539\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.49705 to 2.45391, saving model to best_model.h5\n",
      "Epoch 13/500\n",
      " - 1s - loss: 0.0324 - val_loss: 2.4138\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.45391 to 2.41375, saving model to best_model.h5\n",
      "Epoch 14/500\n",
      " - 1s - loss: 0.0313 - val_loss: 2.3700\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.41375 to 2.36999, saving model to best_model.h5\n",
      "Epoch 15/500\n",
      " - 1s - loss: 0.0286 - val_loss: 2.3282\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.36999 to 2.32819, saving model to best_model.h5\n",
      "Epoch 16/500\n",
      " - 1s - loss: 0.0294 - val_loss: 2.2933\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.32819 to 2.29325, saving model to best_model.h5\n",
      "Epoch 17/500\n",
      " - 1s - loss: 0.0287 - val_loss: 2.2687\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.29325 to 2.26869, saving model to best_model.h5\n",
      "Epoch 18/500\n",
      " - 1s - loss: 0.0285 - val_loss: 2.2480\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.26869 to 2.24805, saving model to best_model.h5\n",
      "Epoch 19/500\n",
      " - 1s - loss: 0.0281 - val_loss: 2.2134\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.24805 to 2.21343, saving model to best_model.h5\n",
      "Epoch 20/500\n",
      " - 1s - loss: 0.0281 - val_loss: 2.1926\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.21343 to 2.19258, saving model to best_model.h5\n",
      "Epoch 21/500\n",
      " - 1s - loss: 0.0286 - val_loss: 2.1598\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.19258 to 2.15976, saving model to best_model.h5\n",
      "Epoch 22/500\n",
      " - 1s - loss: 0.0269 - val_loss: 2.1347\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.15976 to 2.13472, saving model to best_model.h5\n",
      "Epoch 23/500\n",
      " - 1s - loss: 0.0276 - val_loss: 2.1087\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.13472 to 2.10872, saving model to best_model.h5\n",
      "Epoch 24/500\n",
      " - 1s - loss: 0.0249 - val_loss: 2.0856\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.10872 to 2.08564, saving model to best_model.h5\n",
      "Epoch 25/500\n",
      " - 1s - loss: 0.0254 - val_loss: 2.0509\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.08564 to 2.05090, saving model to best_model.h5\n",
      "Epoch 26/500\n",
      " - 1s - loss: 0.0242 - val_loss: 2.0157\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.05090 to 2.01573, saving model to best_model.h5\n",
      "Epoch 27/500\n",
      " - 1s - loss: 0.0236 - val_loss: 2.0057\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.01573 to 2.00573, saving model to best_model.h5\n",
      "Epoch 28/500\n",
      " - 1s - loss: 0.0243 - val_loss: 1.9772\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.00573 to 1.97716, saving model to best_model.h5\n",
      "Epoch 29/500\n",
      " - 1s - loss: 0.0230 - val_loss: 1.9746\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.97716 to 1.97456, saving model to best_model.h5\n",
      "Epoch 30/500\n",
      " - 1s - loss: 0.0247 - val_loss: 1.9552\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.97456 to 1.95521, saving model to best_model.h5\n",
      "Epoch 31/500\n",
      " - 1s - loss: 0.0222 - val_loss: 1.9225\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.95521 to 1.92247, saving model to best_model.h5\n",
      "Epoch 32/500\n",
      " - 1s - loss: 0.0244 - val_loss: 1.9117\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.92247 to 1.91169, saving model to best_model.h5\n",
      "Epoch 33/500\n",
      " - 1s - loss: 0.0240 - val_loss: 1.9136\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.91169\n",
      "Epoch 34/500\n",
      " - 1s - loss: 0.0218 - val_loss: 1.9030\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.91169 to 1.90303, saving model to best_model.h5\n",
      "Epoch 35/500\n",
      " - 1s - loss: 0.0227 - val_loss: 1.9037\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.90303\n",
      "Epoch 36/500\n",
      " - 1s - loss: 0.0221 - val_loss: 1.8778\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.90303 to 1.87781, saving model to best_model.h5\n",
      "Epoch 37/500\n",
      " - 1s - loss: 0.0219 - val_loss: 1.8670\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.87781 to 1.86695, saving model to best_model.h5\n",
      "Epoch 38/500\n",
      " - 1s - loss: 0.0231 - val_loss: 1.8589\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.86695 to 1.85892, saving model to best_model.h5\n",
      "Epoch 39/500\n",
      " - 1s - loss: 0.0235 - val_loss: 1.8613\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.85892\n",
      "Epoch 40/500\n",
      " - 1s - loss: 0.0234 - val_loss: 1.8449\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.85892 to 1.84492, saving model to best_model.h5\n",
      "Epoch 41/500\n",
      " - 1s - loss: 0.0228 - val_loss: 1.8487\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.84492\n",
      "Epoch 42/500\n",
      " - 1s - loss: 0.0234 - val_loss: 1.8387\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.84492 to 1.83871, saving model to best_model.h5\n",
      "Epoch 43/500\n",
      " - 1s - loss: 0.0224 - val_loss: 1.8566\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.83871\n",
      "Epoch 44/500\n",
      " - 1s - loss: 0.0208 - val_loss: 1.8381\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.83871 to 1.83812, saving model to best_model.h5\n",
      "Epoch 45/500\n",
      " - 1s - loss: 0.0207 - val_loss: 1.8364\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.83812 to 1.83644, saving model to best_model.h5\n",
      "Epoch 46/500\n",
      " - 1s - loss: 0.0213 - val_loss: 1.8176\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.83644 to 1.81756, saving model to best_model.h5\n",
      "Epoch 47/500\n",
      " - 1s - loss: 0.0219 - val_loss: 1.8121\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.81756 to 1.81206, saving model to best_model.h5\n",
      "Epoch 48/500\n",
      " - 1s - loss: 0.0210 - val_loss: 1.8256\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.81206\n",
      "Epoch 49/500\n",
      " - 1s - loss: 0.0201 - val_loss: 1.8072\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.81206 to 1.80723, saving model to best_model.h5\n",
      "Epoch 50/500\n",
      " - 1s - loss: 0.0232 - val_loss: 1.8091\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.80723\n",
      "Epoch 51/500\n",
      " - 1s - loss: 0.0209 - val_loss: 1.7995\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.80723 to 1.79953, saving model to best_model.h5\n",
      "Epoch 52/500\n",
      " - 1s - loss: 0.0206 - val_loss: 1.7911\n",
      "\n",
      "Epoch 00052: val_loss improved from 1.79953 to 1.79109, saving model to best_model.h5\n",
      "Epoch 53/500\n",
      " - 1s - loss: 0.0199 - val_loss: 1.7959\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.79109\n",
      "Epoch 54/500\n",
      " - 1s - loss: 0.0219 - val_loss: 1.7782\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.79109 to 1.77815, saving model to best_model.h5\n",
      "Epoch 55/500\n",
      " - 1s - loss: 0.0197 - val_loss: 1.7880\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.77815\n",
      "Epoch 56/500\n",
      " - 1s - loss: 0.0224 - val_loss: 1.7800\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.77815\n",
      "Epoch 57/500\n",
      " - 1s - loss: 0.0208 - val_loss: 1.7779\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.77815 to 1.77787, saving model to best_model.h5\n",
      "Epoch 58/500\n",
      " - 1s - loss: 0.0205 - val_loss: 1.7760\n",
      "\n",
      "Epoch 00058: val_loss improved from 1.77787 to 1.77598, saving model to best_model.h5\n",
      "Epoch 59/500\n",
      " - 1s - loss: 0.0195 - val_loss: 1.7612\n",
      "\n",
      "Epoch 00059: val_loss improved from 1.77598 to 1.76121, saving model to best_model.h5\n",
      "Epoch 60/500\n",
      " - 1s - loss: 0.0191 - val_loss: 1.7646\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.76121\n",
      "Epoch 61/500\n",
      " - 1s - loss: 0.0215 - val_loss: 1.7634\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.76121\n",
      "Epoch 62/500\n",
      " - 1s - loss: 0.0211 - val_loss: 1.7587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00062: val_loss improved from 1.76121 to 1.75873, saving model to best_model.h5\n",
      "Epoch 63/500\n",
      " - 1s - loss: 0.0200 - val_loss: 1.7514\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.75873 to 1.75139, saving model to best_model.h5\n",
      "Epoch 64/500\n",
      " - 1s - loss: 0.0223 - val_loss: 1.7605\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.75139\n",
      "Epoch 65/500\n",
      " - 1s - loss: 0.0213 - val_loss: 1.7608\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.75139\n",
      "Epoch 66/500\n",
      " - 1s - loss: 0.0197 - val_loss: 1.7568\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.75139\n",
      "Epoch 67/500\n",
      " - 1s - loss: 0.0208 - val_loss: 1.7536\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.75139\n",
      "Epoch 68/500\n",
      " - 1s - loss: 0.0194 - val_loss: 1.7548\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.75139\n",
      "Epoch 69/500\n",
      " - 1s - loss: 0.0203 - val_loss: 1.7520\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.75139\n",
      "Epoch 70/500\n",
      " - 1s - loss: 0.0203 - val_loss: 1.7487\n",
      "\n",
      "Epoch 00070: val_loss improved from 1.75139 to 1.74865, saving model to best_model.h5\n",
      "Epoch 71/500\n",
      " - 1s - loss: 0.0197 - val_loss: 1.7413\n",
      "\n",
      "Epoch 00071: val_loss improved from 1.74865 to 1.74132, saving model to best_model.h5\n",
      "Epoch 72/500\n",
      " - 1s - loss: 0.0201 - val_loss: 1.7478\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.74132\n",
      "Epoch 73/500\n",
      " - 1s - loss: 0.0188 - val_loss: 1.7318\n",
      "\n",
      "Epoch 00073: val_loss improved from 1.74132 to 1.73184, saving model to best_model.h5\n",
      "Epoch 74/500\n",
      " - 1s - loss: 0.0199 - val_loss: 1.7410\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.73184\n",
      "Epoch 75/500\n",
      " - 1s - loss: 0.0202 - val_loss: 1.7466\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.73184\n",
      "Epoch 76/500\n",
      " - 1s - loss: 0.0197 - val_loss: 1.7355\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.73184\n",
      "Epoch 77/500\n",
      " - 1s - loss: 0.0194 - val_loss: 1.7487\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.73184\n",
      "Epoch 78/500\n",
      " - 1s - loss: 0.0194 - val_loss: 1.7482\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.73184\n",
      "Epoch 79/500\n",
      " - 1s - loss: 0.0202 - val_loss: 1.7536\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.73184\n",
      "Epoch 80/500\n",
      " - 1s - loss: 0.0210 - val_loss: 1.7461\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.73184\n",
      "Epoch 81/500\n",
      " - 1s - loss: 0.0197 - val_loss: 1.7495\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.73184\n",
      "Epoch 82/500\n",
      " - 1s - loss: 0.0198 - val_loss: 1.7614\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.73184\n",
      "Epoch 83/500\n",
      " - 1s - loss: 0.0183 - val_loss: 1.7505\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.73184\n",
      "Epoch 84/500\n",
      " - 1s - loss: 0.0181 - val_loss: 1.7600\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.73184\n",
      "Epoch 85/500\n",
      " - 1s - loss: 0.0189 - val_loss: 1.7648\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.73184\n",
      "Epoch 86/500\n",
      " - 1s - loss: 0.0194 - val_loss: 1.7692\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.73184\n",
      "Epoch 87/500\n",
      " - 1s - loss: 0.0187 - val_loss: 1.7477\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.73184\n",
      "Epoch 88/500\n",
      " - 1s - loss: 0.0198 - val_loss: 1.7622\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.73184\n",
      "Epoch 89/500\n",
      " - 1s - loss: 0.0187 - val_loss: 1.7526\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.73184\n",
      "Epoch 90/500\n",
      " - 1s - loss: 0.0201 - val_loss: 1.7634\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.73184\n",
      "Epoch 91/500\n",
      " - 1s - loss: 0.0172 - val_loss: 1.7535\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.73184\n",
      "Epoch 92/500\n",
      " - 1s - loss: 0.0190 - val_loss: 1.7641\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.73184\n",
      "Epoch 93/500\n",
      " - 1s - loss: 0.0184 - val_loss: 1.7429\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.73184\n",
      "Epoch 94/500\n",
      " - 1s - loss: 0.0190 - val_loss: 1.7577\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.73184\n",
      "Epoch 95/500\n",
      " - 1s - loss: 0.0200 - val_loss: 1.7544\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.73184\n",
      "Epoch 96/500\n",
      " - 1s - loss: 0.0185 - val_loss: 1.7556\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.73184\n",
      "Epoch 97/500\n",
      " - 1s - loss: 0.0190 - val_loss: 1.7610\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.73184\n",
      "Epoch 98/500\n",
      " - 1s - loss: 0.0193 - val_loss: 1.7542\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.73184\n",
      "Epoch 99/500\n",
      " - 1s - loss: 0.0166 - val_loss: 1.7458\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.73184\n",
      "Epoch 100/500\n",
      " - 1s - loss: 0.0169 - val_loss: 1.7455\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.73184\n",
      "Epoch 101/500\n",
      " - 1s - loss: 0.0168 - val_loss: 1.7351\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.73184\n",
      "Epoch 102/500\n",
      " - 1s - loss: 0.0161 - val_loss: 1.7388\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.73184\n",
      "Epoch 103/500\n",
      " - 1s - loss: 0.0170 - val_loss: 1.7399\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.73184\n",
      "Epoch 104/500\n",
      " - 1s - loss: 0.0183 - val_loss: 1.7327\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.73184\n",
      "Epoch 105/500\n",
      " - 1s - loss: 0.0166 - val_loss: 1.7346\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.73184\n",
      "Epoch 106/500\n",
      " - 1s - loss: 0.0178 - val_loss: 1.7368\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.73184\n",
      "Epoch 107/500\n",
      " - 1s - loss: 0.0182 - val_loss: 1.7335\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.73184\n",
      "Epoch 108/500\n",
      " - 1s - loss: 0.0181 - val_loss: 1.7254\n",
      "\n",
      "Epoch 00108: val_loss improved from 1.73184 to 1.72544, saving model to best_model.h5\n",
      "Epoch 109/500\n",
      " - 1s - loss: 0.0171 - val_loss: 1.7353\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.72544\n",
      "Epoch 110/500\n",
      " - 1s - loss: 0.0180 - val_loss: 1.7304\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.72544\n",
      "Epoch 111/500\n",
      " - 1s - loss: 0.0168 - val_loss: 1.7478\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.72544\n",
      "Epoch 112/500\n",
      " - 1s - loss: 0.0173 - val_loss: 1.7303\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.72544\n",
      "Epoch 113/500\n",
      " - 1s - loss: 0.0179 - val_loss: 1.7258\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.72544\n",
      "Epoch 114/500\n",
      " - 1s - loss: 0.0177 - val_loss: 1.7334\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.72544\n",
      "Epoch 115/500\n",
      " - 1s - loss: 0.0173 - val_loss: 1.7344\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.72544\n",
      "Epoch 116/500\n",
      " - 1s - loss: 0.0175 - val_loss: 1.7328\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.72544\n",
      "Epoch 117/500\n",
      " - 1s - loss: 0.0173 - val_loss: 1.7379\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.72544\n",
      "Epoch 118/500\n",
      " - 1s - loss: 0.0172 - val_loss: 1.7249\n",
      "\n",
      "Epoch 00118: val_loss improved from 1.72544 to 1.72488, saving model to best_model.h5\n",
      "Epoch 119/500\n",
      " - 1s - loss: 0.0158 - val_loss: 1.7345\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.72488\n",
      "Epoch 120/500\n",
      " - 1s - loss: 0.0159 - val_loss: 1.7233\n",
      "\n",
      "Epoch 00120: val_loss improved from 1.72488 to 1.72329, saving model to best_model.h5\n",
      "Epoch 121/500\n",
      " - 1s - loss: 0.0149 - val_loss: 1.7325\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.72329\n",
      "Epoch 122/500\n",
      " - 1s - loss: 0.0176 - val_loss: 1.7340\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.72329\n",
      "Epoch 123/500\n",
      " - 1s - loss: 0.0153 - val_loss: 1.7136\n",
      "\n",
      "Epoch 00123: val_loss improved from 1.72329 to 1.71360, saving model to best_model.h5\n",
      "Epoch 124/500\n",
      " - 1s - loss: 0.0155 - val_loss: 1.7021\n",
      "\n",
      "Epoch 00124: val_loss improved from 1.71360 to 1.70206, saving model to best_model.h5\n",
      "Epoch 125/500\n",
      " - 1s - loss: 0.0169 - val_loss: 1.7228\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.70206\n",
      "Epoch 126/500\n",
      " - 1s - loss: 0.0174 - val_loss: 1.7059\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.70206\n",
      "Epoch 127/500\n",
      " - 1s - loss: 0.0163 - val_loss: 1.7277\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.70206\n",
      "Epoch 128/500\n",
      " - 1s - loss: 0.0151 - val_loss: 1.6965\n",
      "\n",
      "Epoch 00128: val_loss improved from 1.70206 to 1.69650, saving model to best_model.h5\n",
      "Epoch 129/500\n",
      " - 1s - loss: 0.0165 - val_loss: 1.7209\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.69650\n",
      "Epoch 130/500\n",
      " - 1s - loss: 0.0160 - val_loss: 1.7220\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.69650\n",
      "Epoch 131/500\n",
      " - 1s - loss: 0.0155 - val_loss: 1.7228\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.69650\n",
      "Epoch 132/500\n",
      " - 1s - loss: 0.0158 - val_loss: 1.7173\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.69650\n",
      "Epoch 133/500\n",
      " - 1s - loss: 0.0159 - val_loss: 1.7244\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.69650\n",
      "Epoch 134/500\n",
      " - 1s - loss: 0.0144 - val_loss: 1.7192\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.69650\n",
      "Epoch 135/500\n",
      " - 1s - loss: 0.0149 - val_loss: 1.7196\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.69650\n",
      "Epoch 136/500\n",
      " - 1s - loss: 0.0141 - val_loss: 1.6969\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.69650\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.0160 - val_loss: 1.7326\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.69650\n",
      "Epoch 138/500\n",
      " - 1s - loss: 0.0136 - val_loss: 1.7241\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.69650\n",
      "Epoch 139/500\n",
      " - 1s - loss: 0.0166 - val_loss: 1.7453\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.69650\n",
      "Epoch 140/500\n",
      " - 1s - loss: 0.0154 - val_loss: 1.7029\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.69650\n",
      "Epoch 141/500\n",
      " - 1s - loss: 0.0144 - val_loss: 1.7005\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.69650\n",
      "Epoch 142/500\n",
      " - 1s - loss: 0.0165 - val_loss: 1.7193\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.69650\n",
      "Epoch 143/500\n",
      " - 1s - loss: 0.0164 - val_loss: 1.7021\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.69650\n",
      "Epoch 144/500\n",
      " - 1s - loss: 0.0134 - val_loss: 1.7029\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.69650\n",
      "Epoch 145/500\n",
      " - 1s - loss: 0.0140 - val_loss: 1.7057\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.69650\n",
      "Epoch 146/500\n",
      " - 1s - loss: 0.0149 - val_loss: 1.6956\n",
      "\n",
      "Epoch 00146: val_loss improved from 1.69650 to 1.69565, saving model to best_model.h5\n",
      "Epoch 147/500\n",
      " - 1s - loss: 0.0142 - val_loss: 1.6938\n",
      "\n",
      "Epoch 00147: val_loss improved from 1.69565 to 1.69382, saving model to best_model.h5\n",
      "Epoch 148/500\n",
      " - 1s - loss: 0.0147 - val_loss: 1.7066\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.69382\n",
      "Epoch 149/500\n",
      " - 1s - loss: 0.0145 - val_loss: 1.7049\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.69382\n",
      "Epoch 150/500\n",
      " - 1s - loss: 0.0151 - val_loss: 1.7218\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.69382\n",
      "Epoch 151/500\n",
      " - 1s - loss: 0.0135 - val_loss: 1.6997\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.69382\n",
      "Epoch 152/500\n",
      " - 1s - loss: 0.0148 - val_loss: 1.7292\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.69382\n",
      "Epoch 153/500\n",
      " - 1s - loss: 0.0132 - val_loss: 1.7270\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.69382\n",
      "Epoch 154/500\n",
      " - 1s - loss: 0.0129 - val_loss: 1.7059\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.69382\n",
      "Epoch 155/500\n",
      " - 1s - loss: 0.0135 - val_loss: 1.7097\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.69382\n",
      "Epoch 156/500\n",
      " - 1s - loss: 0.0144 - val_loss: 1.7245\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.69382\n",
      "Epoch 157/500\n",
      " - 1s - loss: 0.0135 - val_loss: 1.6947\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.69382\n",
      "Epoch 158/500\n",
      " - 1s - loss: 0.0134 - val_loss: 1.7188\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.69382\n",
      "Epoch 159/500\n",
      " - 1s - loss: 0.0136 - val_loss: 1.7180\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.69382\n",
      "Epoch 160/500\n",
      " - 1s - loss: 0.0134 - val_loss: 1.7182\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.69382\n",
      "Epoch 161/500\n",
      " - 1s - loss: 0.0124 - val_loss: 1.7155\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.69382\n",
      "Epoch 162/500\n",
      " - 1s - loss: 0.0118 - val_loss: 1.6989\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.69382\n",
      "Epoch 163/500\n",
      " - 1s - loss: 0.0136 - val_loss: 1.7288\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.69382\n",
      "Epoch 164/500\n",
      " - 1s - loss: 0.0112 - val_loss: 1.7016\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.69382\n",
      "Epoch 165/500\n",
      " - 1s - loss: 0.0148 - val_loss: 1.7079\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.69382\n",
      "Epoch 166/500\n",
      " - 1s - loss: 0.0119 - val_loss: 1.6792\n",
      "\n",
      "Epoch 00166: val_loss improved from 1.69382 to 1.67923, saving model to best_model.h5\n",
      "Epoch 167/500\n",
      " - 1s - loss: 0.0125 - val_loss: 1.6921\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.67923\n",
      "Epoch 168/500\n",
      " - 1s - loss: 0.0139 - val_loss: 1.6794\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.67923\n",
      "Epoch 169/500\n",
      " - 1s - loss: 0.0123 - val_loss: 1.7001\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.67923\n",
      "Epoch 170/500\n",
      " - 1s - loss: 0.0110 - val_loss: 1.6833\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.67923\n",
      "Epoch 171/500\n",
      " - 1s - loss: 0.0123 - val_loss: 1.7232\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.67923\n",
      "Epoch 172/500\n",
      " - 1s - loss: 0.0115 - val_loss: 1.7001\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.67923\n",
      "Epoch 173/500\n",
      " - 1s - loss: 0.0116 - val_loss: 1.7357\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.67923\n",
      "Epoch 174/500\n",
      " - 1s - loss: 0.0131 - val_loss: 1.6997\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.67923\n",
      "Epoch 175/500\n",
      " - 1s - loss: 0.0114 - val_loss: 1.7403\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.67923\n",
      "Epoch 176/500\n",
      " - 1s - loss: 0.0105 - val_loss: 1.7132\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.67923\n",
      "Epoch 177/500\n",
      " - 1s - loss: 0.0139 - val_loss: 1.7665\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.67923\n",
      "Epoch 178/500\n",
      " - 1s - loss: 0.0116 - val_loss: 1.7134\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.67923\n",
      "Epoch 179/500\n",
      " - 1s - loss: 0.0126 - val_loss: 1.7266\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.67923\n",
      "Epoch 180/500\n",
      " - 1s - loss: 0.0127 - val_loss: 1.6879\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.67923\n",
      "Epoch 181/500\n",
      " - 1s - loss: 0.0107 - val_loss: 1.7258\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.67923\n",
      "Epoch 182/500\n",
      " - 1s - loss: 0.0133 - val_loss: 1.6993\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.67923\n",
      "Epoch 183/500\n",
      " - 1s - loss: 0.0112 - val_loss: 1.7316\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.67923\n",
      "Epoch 184/500\n",
      " - 1s - loss: 0.0118 - val_loss: 1.7205\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.67923\n",
      "Epoch 185/500\n",
      " - 1s - loss: 0.0113 - val_loss: 1.7435\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.67923\n",
      "Epoch 186/500\n",
      " - 1s - loss: 0.0111 - val_loss: 1.7618\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.67923\n",
      "Epoch 187/500\n",
      " - 1s - loss: 0.0113 - val_loss: 1.7255\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.67923\n",
      "Epoch 188/500\n",
      " - 1s - loss: 0.0132 - val_loss: 1.7213\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.67923\n",
      "Epoch 189/500\n",
      " - 1s - loss: 0.0113 - val_loss: 1.7243\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.67923\n",
      "Epoch 190/500\n",
      " - 1s - loss: 0.0101 - val_loss: 1.7295\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.67923\n",
      "Epoch 191/500\n",
      " - 1s - loss: 0.0142 - val_loss: 1.7147\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.67923\n",
      "Epoch 192/500\n",
      " - 1s - loss: 0.0111 - val_loss: 1.7772\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.67923\n",
      "Epoch 193/500\n",
      " - 1s - loss: 0.0112 - val_loss: 1.7864\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.67923\n",
      "Epoch 194/500\n",
      " - 1s - loss: 0.0115 - val_loss: 1.7350\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.67923\n",
      "Epoch 195/500\n",
      " - 1s - loss: 0.0119 - val_loss: 1.7491\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.67923\n",
      "Epoch 196/500\n",
      " - 1s - loss: 0.0102 - val_loss: 1.7627\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.67923\n",
      "Epoch 197/500\n",
      " - 1s - loss: 0.0115 - val_loss: 1.7806\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.67923\n",
      "Epoch 198/500\n",
      " - 1s - loss: 0.0129 - val_loss: 1.7801\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.67923\n",
      "Epoch 199/500\n",
      " - 1s - loss: 0.0104 - val_loss: 1.7772\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.67923\n",
      "Epoch 200/500\n",
      " - 1s - loss: 0.0124 - val_loss: 1.8289\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.67923\n",
      "Epoch 201/500\n",
      " - 1s - loss: 0.0107 - val_loss: 1.7812\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.67923\n",
      "Epoch 202/500\n",
      " - 1s - loss: 0.0108 - val_loss: 1.7999\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.67923\n",
      "Epoch 203/500\n",
      " - 1s - loss: 0.0112 - val_loss: 1.8010\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.67923\n",
      "Epoch 204/500\n",
      " - 1s - loss: 0.0106 - val_loss: 1.8286\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.67923\n",
      "Epoch 205/500\n",
      " - 1s - loss: 0.0100 - val_loss: 1.7964\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.67923\n",
      "Epoch 206/500\n",
      " - 1s - loss: 0.0115 - val_loss: 1.8059\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.67923\n",
      "Epoch 00206: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),\n",
    "                    dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
    "                    kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(60, dropout=0.0))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "    \n",
    "mcp = ModelCheckpoint(os.path.join(\"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "                          save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "    \n",
    "history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error is 2.1969629402141755 (10,) (10,)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Mean Squared Error is\", error, y_pred.shape, y_test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_org = (y_pred * min_max_scaler.data_range_[0]) + min_max_scaler.data_min_[0]\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[0]) + min_max_scaler.data_min_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16db4301988>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class YahooFinanceHistory:\n",
    "    timeout = 2\n",
    "    crumb_link = 'https://finance.yahoo.com/quote/{0}/history?p={0}'\n",
    "    crumble_regex = r'CrumbStore\":{\"crumb\":\"(.*?)\"}'\n",
    "    quote_link = 'https://query1.finance.yahoo.com/v7/finance/download/{quote}?period1={dfrom}&period2={dto}&interval=1d&events=history&crumb={crumb}'\n",
    "\n",
    "    def __init__(self, symbol, days_back=7):\n",
    "        self.symbol = symbol\n",
    "        self.session = requests.Session()\n",
    "        self.dt = timedelta(days=days_back)\n",
    "\n",
    "    def get_crumb(self):\n",
    "        response = self.session.get(self.crumb_link.format(self.symbol), timeout=self.timeout)\n",
    "        response.raise_for_status()\n",
    "        match = re.search(self.crumble_regex, response.text)\n",
    "        if not match:\n",
    "            raise ValueError('Could not get crumb from Yahoo Finance')\n",
    "        else:\n",
    "            self.crumb = match.group(1)\n",
    "\n",
    "    def get_quote(self):\n",
    "        \n",
    "        if not hasattr(self, 'crumb') or len(self.session.cookies) == 0:\n",
    "            self.get_crumb()\n",
    "        now = datetime.utcnow()\n",
    "        dateto = int(now.timestamp())\n",
    "        datefrom = int((now - self.dt).timestamp())\n",
    "        url = self.quote_link.format(quote=self.symbol, dfrom=datefrom, dto=dateto, crumb=self.crumb)\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(StringIO(response.text), parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ge = YahooFinanceHistory('GE', days_back=15000).get_quote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979-03-14</td>\n",
       "      <td>0.944010</td>\n",
       "      <td>0.949018</td>\n",
       "      <td>0.941506</td>\n",
       "      <td>0.944010</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>2960200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979-03-15</td>\n",
       "      <td>0.944010</td>\n",
       "      <td>0.956530</td>\n",
       "      <td>0.944010</td>\n",
       "      <td>0.949018</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>5191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979-03-16</td>\n",
       "      <td>0.949018</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.944010</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>7418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1979-03-19</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.971554</td>\n",
       "      <td>0.956530</td>\n",
       "      <td>0.966546</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>5381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979-03-20</td>\n",
       "      <td>0.966546</td>\n",
       "      <td>0.969050</td>\n",
       "      <td>0.956530</td>\n",
       "      <td>0.959034</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>4662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10351</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>7.870000</td>\n",
       "      <td>8.180000</td>\n",
       "      <td>7.820000</td>\n",
       "      <td>7.940000</td>\n",
       "      <td>7.940000</td>\n",
       "      <td>121149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10352</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>7.550000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.040000</td>\n",
       "      <td>7.040000</td>\n",
       "      <td>99556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10353</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>7.020000</td>\n",
       "      <td>7.390000</td>\n",
       "      <td>6.770000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>97498400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10354</th>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>7.030000</td>\n",
       "      <td>7.050000</td>\n",
       "      <td>6.580000</td>\n",
       "      <td>6.730000</td>\n",
       "      <td>6.730000</td>\n",
       "      <td>97803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>7.070000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>6.940000</td>\n",
       "      <td>7.230000</td>\n",
       "      <td>7.230000</td>\n",
       "      <td>97912000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10356 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date      Open      High       Low     Close  Adj Close     Volume\n",
       "0     1979-03-14  0.944010  0.949018  0.941506  0.944010   0.003767    2960200\n",
       "1     1979-03-15  0.944010  0.956530  0.944010  0.949018   0.003787    5191600\n",
       "2     1979-03-16  0.949018  0.961538  0.944010  0.961538   0.003837    7418100\n",
       "3     1979-03-19  0.961538  0.971554  0.956530  0.966546   0.003857    5381300\n",
       "4     1979-03-20  0.966546  0.969050  0.956530  0.959034   0.003827    4662500\n",
       "...          ...       ...       ...       ...       ...        ...        ...\n",
       "10351 2020-03-31  7.870000  8.180000  7.820000  7.940000   7.940000  121149900\n",
       "10352 2020-04-01  7.520000  7.550000  7.000000  7.040000   7.040000   99556900\n",
       "10353 2020-04-02  7.020000  7.390000  6.770000  6.900000   6.900000   97498400\n",
       "10354 2020-04-03  7.030000  7.050000  6.580000  6.730000   6.730000   97803300\n",
       "10355 2020-04-06  7.070000  7.300000  6.940000  7.230000   7.230000   97912000\n",
       "\n",
       "[10356 rows x 7 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 20,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.00010000,\n",
    "    \"time_steps\": 60\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat,batch_size):\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if no_of_rows_drop > 0:\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index):\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    print(\"dim_0\",dim_0)\n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = [\"Close\"]\n",
    "df_train, df_test = train_test_split(df_ge, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_0 8224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e513916c4d4676a1e6db90eb524f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8224.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_t, y_t = build_timeseries(x_train, 0)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_0 2012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744623ba599f4fe2823421b5e4a5bc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2012.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 0)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8220 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      " - 47s - loss: 0.0274 - val_loss: 0.0142\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01422, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      " - 45s - loss: 0.0034 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.01422\n",
      "Epoch 3/50\n",
      " - 45s - loss: 0.0023 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01422\n",
      "Epoch 4/50\n",
      " - 46s - loss: 0.0020 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01422\n",
      "Epoch 5/50\n",
      " - 47s - loss: 0.0020 - val_loss: 0.0150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01422\n",
      "Epoch 6/50\n",
      " - 44s - loss: 0.0018 - val_loss: 0.0130\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01422 to 0.01304, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      " - 45s - loss: 0.0019 - val_loss: 0.0126\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01304 to 0.01260, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      " - 49s - loss: 0.0018 - val_loss: 0.0143\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01260\n",
      "Epoch 9/50\n",
      " - 48s - loss: 0.0017 - val_loss: 0.0145\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01260\n",
      "Epoch 10/50\n",
      " - 48s - loss: 0.0016 - val_loss: 0.0126\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01260 to 0.01259, saving model to best_model.h5\n",
      "Epoch 11/50\n",
      " - 50s - loss: 0.0016 - val_loss: 0.0134\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01259\n",
      "Epoch 12/50\n",
      " - 50s - loss: 0.0015 - val_loss: 0.0111\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01259 to 0.01109, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      " - 50s - loss: 0.0015 - val_loss: 0.0085\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01109 to 0.00846, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      " - 50s - loss: 0.0015 - val_loss: 0.0084\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00846 to 0.00836, saving model to best_model.h5\n",
      "Epoch 15/50\n",
      " - 52s - loss: 0.0014 - val_loss: 0.0080\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00836 to 0.00796, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      " - 50s - loss: 0.0014 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00796 to 0.00574, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      " - 51s - loss: 0.0013 - val_loss: 0.0044\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00574 to 0.00437, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      " - 48s - loss: 0.0013 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00437\n",
      "Epoch 19/50\n",
      " - 59s - loss: 0.0012 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00437 to 0.00433, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      " - 54s - loss: 0.0012 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00433 to 0.00373, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      " - 53s - loss: 0.0012 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00373\n",
      "Epoch 22/50\n",
      " - 55s - loss: 0.0011 - val_loss: 0.0031\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00373 to 0.00308, saving model to best_model.h5\n",
      "Epoch 23/50\n",
      " - 53s - loss: 0.0011 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00308 to 0.00289, saving model to best_model.h5\n",
      "Epoch 24/50\n",
      " - 52s - loss: 0.0011 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00289 to 0.00267, saving model to best_model.h5\n",
      "Epoch 25/50\n",
      " - 56s - loss: 0.0011 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00267 to 0.00258, saving model to best_model.h5\n",
      "Epoch 26/50\n",
      " - 52s - loss: 0.0011 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00258 to 0.00251, saving model to best_model.h5\n",
      "Epoch 27/50\n",
      " - 55s - loss: 0.0011 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00251\n",
      "Epoch 28/50\n",
      " - 55s - loss: 9.7942e-04 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00251 to 0.00238, saving model to best_model.h5\n",
      "Epoch 29/50\n",
      " - 52s - loss: 9.6504e-04 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00238 to 0.00213, saving model to best_model.h5\n",
      "Epoch 30/50\n",
      " - 51s - loss: 9.8553e-04 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00213 to 0.00205, saving model to best_model.h5\n",
      "Epoch 31/50\n",
      " - 55s - loss: 9.2484e-04 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00205 to 0.00179, saving model to best_model.h5\n",
      "Epoch 32/50\n",
      " - 53s - loss: 9.3819e-04 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00179 to 0.00178, saving model to best_model.h5\n",
      "Epoch 33/50\n",
      " - 52s - loss: 9.2218e-04 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00178 to 0.00178, saving model to best_model.h5\n",
      "Epoch 34/50\n",
      " - 53s - loss: 8.2311e-04 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00178\n",
      "Epoch 35/50\n",
      " - 51s - loss: 8.1698e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00178 to 0.00147, saving model to best_model.h5\n",
      "Epoch 36/50\n",
      " - 52s - loss: 8.0940e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00147 to 0.00146, saving model to best_model.h5\n",
      "Epoch 37/50\n",
      " - 57s - loss: 7.9594e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00146\n",
      "Epoch 38/50\n",
      " - 51s - loss: 7.7783e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00146 to 0.00133, saving model to best_model.h5\n",
      "Epoch 39/50\n",
      " - 54s - loss: 7.8784e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00133 to 0.00127, saving model to best_model.h5\n",
      "Epoch 40/50\n",
      " - 52s - loss: 7.3853e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00127\n",
      "Epoch 41/50\n",
      " - 41s - loss: 7.2719e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00127\n",
      "Epoch 42/50\n",
      " - 41s - loss: 6.9668e-04 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00127\n",
      "Epoch 43/50\n",
      " - 41s - loss: 7.1143e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00127 to 0.00116, saving model to best_model.h5\n",
      "Epoch 44/50\n",
      " - 41s - loss: 6.9032e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00116 to 0.00103, saving model to best_model.h5\n",
      "Epoch 45/50\n",
      " - 41s - loss: 6.8566e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00103\n",
      "Epoch 46/50\n",
      " - 41s - loss: 6.8848e-04 - val_loss: 9.7156e-04\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00103 to 0.00097, saving model to best_model.h5\n",
      "Epoch 47/50\n",
      " - 41s - loss: 6.2219e-04 - val_loss: 9.5654e-04\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00097 to 0.00096, saving model to best_model.h5\n",
      "Epoch 48/50\n",
      " - 40s - loss: 6.3762e-04 - val_loss: 8.4987e-04\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00096 to 0.00085, saving model to best_model.h5\n",
      "Epoch 49/50\n",
      " - 41s - loss: 6.4782e-04 - val_loss: 7.8051e-04\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00085 to 0.00078, saving model to best_model.h5\n",
      "Epoch 50/50\n",
      " - 40s - loss: 6.4430e-04 - val_loss: 7.1714e-04\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00078 to 0.00072, saving model to best_model.h5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),\n",
    "                    dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
    "                    kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(60, dropout=0.0))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "    \n",
    "mcp = ModelCheckpoint(os.path.join(\"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "                          save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "    \n",
    "history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error is 0.0010670566073682428 (1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Mean Squared Error is\", error, y_pred.shape, y_test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_org = (y_pred * min_max_scaler.data_range_[0]) + min_max_scaler.data_min_[0]\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[0]) + min_max_scaler.data_min_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16db9f62cc8>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
